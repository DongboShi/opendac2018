{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import sparse\n",
    "import multiprocessing as mlp\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.metrics import calinski_harabaz_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#todo: 还有其他词干抽取器\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "assignments_train_path = './data/assignment_train.json'\n",
    "pubs_train_path = './data/pubs_train.json'\n",
    "pubs_validate_path = './data/pubs_validate.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_select(X):\n",
    "    opt_score = 0\n",
    "    opt_k = 200\n",
    "    for k in range(200, 401, 10):\n",
    "        m = KMeans(n_clusters=k)\n",
    "        y_pred = m.fit_predict(X)\n",
    "        t = calinski_harabaz_score(X, y_pred)\n",
    "        if t>opt_score:\n",
    "            opt_score = t\n",
    "            opt_k = k\n",
    "    return opt_k  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_fea(docs):\n",
    "    if sum([len(doc) for doc in docs])==0:\n",
    "        return np.zeros((len(docs),1))\n",
    "    v = CountVectorizer()\n",
    "    t = TfidfTransformer()\n",
    "    return t.fit_transform(v.fit_transform(docs))\n",
    "\n",
    "def is_same_name(s1, s2):\n",
    "    return re.sub('[^a-z]', '', s1.lower())==re.sub('[^a-z]', '', s2.lower())\n",
    "\n",
    "def construct_docs4aut(pubs, author, field):\n",
    "    papers = pubs[author]\n",
    "    if field=='org':\n",
    "        #lower_docs = [ aut['org'].lower() for p in papers for aut in p['authors'] if is_same_name(aut['name'], author) ]\n",
    "        \n",
    "        lower_docs=[]\n",
    "        for p in papers:\n",
    "            for aut in p['authors']:\n",
    "                if is_same_name(aut['name'], author):\n",
    "                    lower_docs.append(aut['org'].lower())\n",
    "                    #即使是一篇文章，作者列表也可能存在重名的情况，这时，仅取第一个。\n",
    "                    break\n",
    "    elif field=='keywords':\n",
    "        lower_docs = [' '.join(p[field]).lower()if field in p and p[field]!=None else '' for p in papers]\n",
    "    else:\n",
    "        lower_docs = [p[field].lower() if field in p and p[field]!=None else '' for p in papers ]\n",
    "    #仅保留字母、下划线、空格\n",
    "    docs = [re.sub('[^ _a-z]', '', s) for s in lower_docs]\n",
    "    \n",
    "    #空格分词\n",
    "    splt = [re.split(' +', s) for s in docs]\n",
    "    \n",
    "    #去停用词\n",
    "    stop_words_set = set(stopwords.words('english'))\n",
    "    splt = [[s for s in ss if len(s)>0 and s not in stop_words_set] for ss in splt]\n",
    "    \n",
    "    #抽取词干、合并分词结果\n",
    "    stemer = PorterStemmer()\n",
    "    stem_docs = [' '.join([stemer.stem(s) for s in ss])for ss in splt]\n",
    "    return stem_docs\n",
    "    \n",
    "def cluster4aut(author):\n",
    "    papers = pubs[author]\n",
    "    year_fea = np.array([p['year'] if 'year' in p else 0 for p in papers]).reshape((-1,1))\n",
    "    ids = [p['id'] for p in papers]\n",
    "    \n",
    "    keywords = construct_docs4aut(pubs, author, 'keywords')\n",
    "    keywords_fea = tfidf_fea(keywords)\n",
    "    \n",
    "    abstract = construct_docs4aut(pubs, author, 'abstract')\n",
    "    abs_fea = tfidf_fea(abstract)\n",
    "    \n",
    "    title = construct_docs4aut(pubs, author, 'title')\n",
    "    tit_fea = tfidf_fea(title)\n",
    "    \n",
    "    org = construct_docs4aut(pubs, author, 'org')\n",
    "    org_fea = tfidf_fea(org)\n",
    "    \n",
    "    venue = construct_docs4aut(pubs, author, 'venue')\n",
    "    venue_fea = tfidf_fea(venue)\n",
    "\n",
    "    #print(keywords_fea.shape, abs_fea.shape, tit_fea.shape, org_fea.shape, venue_fea.shape, year_fea.shape)\n",
    "    all_fea = sparse.hstack([keywords_fea, abs_fea, tit_fea, org_fea, venue_fea, year_fea])\n",
    "    \n",
    "    sca = MinMaxScaler((0,1))\n",
    "    X = sca.fit_transform(all_fea.toarray())\n",
    "    \n",
    "    N_cluster = model_select(X)\n",
    "    m = KMeans(n_clusters = N_cluster)\n",
    "    m.fit(X)\n",
    "    \n",
    "    res = {}\n",
    "    for i in range(len(m.labels_)):\n",
    "        res.setdefault(m.labels_[i], []).append(ids[i])\n",
    "    \n",
    "    return list(res.values())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assignments_train = json.load(open(assignments_train_path,'r'))\n",
    "pubs_train = json.load(open(pubs_train_path, 'r'))\n",
    "pubs_validate = json.load(open(pubs_validate_path,'r'))\n",
    "pubs={**pubs_train, **pubs_validate}\n",
    "assert(len(pubs)==len(pubs_train)+len(pubs_validate))\n",
    "pool = mlp.Pool(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = pool.map(cluster4aut, list(pubs_train.keys()))\n",
    "assert(len(pubs_train)==len(train_results))\n",
    "json.dump(dict(zip( pubs_train.keys(), train_results )), open('assignment_train_result.json', 'w')  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validate_results = pool.map(cluster4aut, list(pubs_validate.keys()))\n",
    "assert(len(pubs_validate)==len(validate_results))\n",
    "json.dump(dict(zip( pubs_validate.keys(), validate_results )), open('assignment_validate_result.json', 'w')  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
