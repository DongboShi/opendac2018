{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "## Imports\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import sparse\n",
    "import multiprocessing as mlp\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.metrics import calinski_harabaz_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.stem.porter import PorterStemmer  #todo: 还有其他词干抽取器\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import pickle as pkl\n",
    "import gc\n",
    "import os\n",
    "from itertools import chain\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "\n",
    "assignments_train_path = './data/assignment_train.json'\n",
    "pubs_train_path = './data/pubs_train.json'\n",
    "pubs_validate_path = './data/pubs_validate.json'\n",
    "stop_words_path = './data/stop_words.txt'\n",
    "\n",
    "## 中间输出文件\n",
    "material_path = './output/material.pkl'                      # doc_id  -> [word1, word2, ...], list\n",
    "word2vect_model_path = './output/word.emb'                   # word2vec model.  usage: Word2Vec.load(...)\n",
    "idf_path = './output/idf.pkl'                                # word    -> idf value, float\n",
    "weighted_embedding_path = './output/weighted_embedding.pkl'  # doc_id  -> X_i, np.ndarray\n",
    "triple_set = './output/triple.pkl'                           # 'emb'   -> anchors; 'emb_pos': positive weighted embedding; 'emb_neg': negative ones\n",
    "global_embedding_path = './output/global_output.pkl'         # doc_id  -> Y_i, np.ndarray\n",
    "\n",
    "## 直接调用: weighted_embedding(), 会返回material, word2vect_model, idf, X_i四元组。\n",
    "#Y_i的读global_embedding_path\n",
    "\n",
    "\n",
    "EMBEDDING_DIM = 100\n",
    "with open(stop_words_path,'r') as f:\n",
    "    s = set(f.readline().split(','))\n",
    "stop_word_list = s.union(stopwords.words('english'))\n",
    "    \n",
    "\n",
    "\n",
    "## Read Data\n",
    "def read_data():\n",
    "    assignments_train = json.load(open(assignments_train_path,'r'))\n",
    "    pubs_train = json.load(open(pubs_train_path, 'r'))\n",
    "    pubs_validate = json.load(open(pubs_validate_path,'r'))\n",
    "    pubs={**pubs_train, **pubs_validate}\n",
    "    assert(len(pubs)==len(pubs_train)+len(pubs_validate))\n",
    "    print('read done!')\n",
    "    return assignments_train, pubs_train, pubs_validate, pubs\n",
    "\n",
    "\n",
    "## Word2Vec\n",
    "def clean_name(nm):\n",
    "    return re.sub('[^a-z]', '', nm.lower())\n",
    "\n",
    "def is_same_name(s1, s2):\n",
    "    return clean_name(s1)==clean_name(s2)\n",
    "\n",
    "def clean_sent(s, prefix):\n",
    "    '''\n",
    "    为区别各字段，不同字段前的词加不同的前缀\n",
    "    '''\n",
    "    words = re.sub('[^ \\-_a-z]', ' ', s.lower()).split()\n",
    "    stemer = PorterStemmer()\n",
    "    return [ '__%s__%s'%(prefix, stemer.stem(w)) for w in words if w not in stop_word_list]\n",
    "    \n",
    "def ExtractTxt(doc, primary_author):\n",
    "    \"\"\"\n",
    "    把一个文档变为：\n",
    "    [题目，合作者(姓名,组织)，期刊，摘要，关键词]\n",
    "    各种预处理之后的word list\n",
    "    \"\"\"\n",
    "    title = clean_sent(doc['title'], 'T') if doc.get('title',None) else []\n",
    "    venue = clean_sent(doc['venue'], 'V') if doc.get('venue',None) else []\n",
    "    abstract = clean_sent(doc['abstract'], 'A') if doc.get('abstract',None) else []\n",
    "    keywords = clean_sent( ' '.join(doc['keywords']), 'K') if doc.get('keywords',None) else []\n",
    "    coauthors = []\n",
    "    if doc.get('authors',None):\n",
    "        for aut in doc['authors']:\n",
    "            if not is_same_name(  aut.get('name',''), primary_author ):\n",
    "                coauthors.append( clean_name(aut.get('name','')) )\n",
    "                coauthors.extend( clean_sent(aut.get('org',''), 'O') )\n",
    "    return title+coauthors+venue+abstract+keywords\n",
    "    \n",
    "def word_embedding():\n",
    "    if os.path.exists(word2vect_model_path) and os.path.exists(material_path):\n",
    "        model = Word2Vec.load(word2vect_model_path)\n",
    "        docs = pkl.load(open(material_path,'rb'))\n",
    "        return model, docs\n",
    "    \n",
    "    material = []\n",
    "    paper_id = []\n",
    "    pool = mlp.Pool(20)\n",
    "    for k,v in pubs.items():\n",
    "        material.extend(pool.starmap( ExtractTxt, zip( v, [k]*len(v) ) ))\n",
    "        paper_id.extend( [doc['id'] for doc in v])\n",
    "    model = Word2Vec(material, size=EMBEDDING_DIM, window=5, min_count=5, workers=20)\n",
    "    docs = dict(zip(paper_id, material))\n",
    "    pkl.dump(docs, open(material_path,'wb'))\n",
    "    model.save(word2vect_model_path)\n",
    "    pool.close()\n",
    "    return model, docs \n",
    "\n",
    "\n",
    "## Weighted Embedding\n",
    "#todo: 并行\n",
    "def calc_idf(material):\n",
    "    if os.path.exists(idf_path):\n",
    "        return pkl.load(open(idf_path,'rb'))\n",
    "    cnt = defaultdict(int)\n",
    "    idf = {}\n",
    "    for doc in material:\n",
    "        for word in doc:\n",
    "            cnt[word]+=1\n",
    "    for k,v in cnt.items():\n",
    "        idf[k] = math.log( len(material)/v )\n",
    "    pkl.dump(idf, open(idf_path, 'wb'))\n",
    "    return idf\n",
    "\n",
    "def project_embedding(docs, wv, idf):\n",
    "    if os.path.exists(weighted_embedding_path):\n",
    "        return pkl.load(open(weighted_embedding_path, 'rb'))\n",
    "\n",
    "    wei_embed = {}\n",
    "    for id, doc in docs.items():\n",
    "        word_vecs = []\n",
    "        sum_weight = 0.0\n",
    "        for word in doc:\n",
    "            if word in wv and word in idf:\n",
    "                word_vecs.append( wv[word] * idf[word] )\n",
    "                sum_weight += idf[word]\n",
    "        wei_embed[id] = np.sum(word_vecs, axis = 0) / sum_weight\n",
    "    pkl.dump(wei_embed, open(weighted_embedding_path, 'wb'))\n",
    "    return wei_embed\n",
    "\n",
    "#得到 X_i, 这部分还算快， 没有写缓存和并行\n",
    "#warning: 加权结果可能有点大。\n",
    "def weighted_embedding():\n",
    "    model, docs = word_embedding()\n",
    "    print('word embedding done!')\n",
    "    idf = calc_idf(docs.values())\n",
    "    weighted = project_embedding(docs, model.wv, idf)\n",
    "    print('weighted embedding done!')\n",
    "    return docs, idf, model, weighted\n",
    "\n",
    "\n",
    "## Generate Triplet Training Data\n",
    "def get_neg_id(all_papers, excludes):\n",
    "    while True:\n",
    "        i = np.random.choice(len(all_papers))\n",
    "        if all_papers[i] not in excludes:\n",
    "            return all_papers[i]\n",
    "\n",
    "\n",
    "def gen_triple(weighted, sz = 1000000):\n",
    "    if os.path.exists(triple_set):\n",
    "        d = pkl.load(open(triple_set,'rb'))\n",
    "        return d['emb'], d['emb_pos'], d['emb_neg']\n",
    "    \n",
    "    triples = []\n",
    "    authors = list(assignments_train.keys())\n",
    "    all_papers = list(set([p['id'] for k,v in pubs_train.items() for p in v]))\n",
    "    I = 0\n",
    "    try:\n",
    "        while True:\n",
    "            author_papers = assignments_train[ authors[I] ]\n",
    "            I+=1\n",
    "            if I>=len(authors):\n",
    "                I=0\n",
    "            for clust in author_papers:\n",
    "                if len(clust)<=1:\n",
    "                    continue\n",
    "                for pid in clust:\n",
    "                    sam = np.random.choice(clust,  min(len(clust), 6), replace=False)\n",
    "                    for pid_pos in sam:\n",
    "                        triples.append( [pid, pid_pos, get_neg_id(all_papers, clust)] )\n",
    "                        if len(triples)>=sz:\n",
    "                            raise StopIteration\n",
    "    except StopIteration as e:\n",
    "        print(len(triples))\n",
    "    \n",
    "    emb = np.array([ weighted[t[0]] for t in triples ])\n",
    "    emb_pos = np.array([ weighted[t[1]] for t in triples ])\n",
    "    emb_neg = np.array([ weighted[t[2]] for t in triples ])\n",
    "    pkl.dump({'emb':emb, 'emb_pos':emb_pos, 'emb_neg':emb_neg}, open(triple_set,'wb'))\n",
    "    return emb, emb_pos, emb_neg\n",
    "\n",
    "\n",
    "## Triplet Model\n",
    "def l2Norm(x):\n",
    "    return K.l2_normalize(x, axis=-1)\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))\n",
    "\n",
    "def triplet_loss(_, y_pred):\n",
    "    margin = K.constant(1)\n",
    "    return K.mean(K.maximum(K.constant(0), K.square(y_pred[:,0,0]) - K.square(y_pred[:,1,0]) + margin))\n",
    "\n",
    "def accuracy(_, y_pred):\n",
    "    return K.mean(y_pred[:,0,0] < y_pred[:,1,0])\n",
    "\n",
    "class GlobalModel(object):\n",
    "    def __init__(self):\n",
    "        self.save_path = './output/GlobalModel.h5'\n",
    "        emb_anchor = Input(shape=(EMBEDDING_DIM, ), name='anchor_input')\n",
    "        emb_pos = Input(shape=(EMBEDDING_DIM, ), name='pos_input')\n",
    "        emb_neg = Input(shape=(EMBEDDING_DIM, ), name='neg_input')\n",
    "\n",
    "        # shared layers\n",
    "        layer1 = Dense(128, activation='relu', name='first_emb_layer')\n",
    "        layer2 = Dense(64, activation='relu', name='last_emb_layer')\n",
    "        norm_layer = Lambda(l2Norm, name='norm_layer', output_shape=[64])\n",
    "\n",
    "        encoded_emb = norm_layer(layer2(layer1(emb_anchor)))\n",
    "        encoded_emb_pos = norm_layer(layer2(layer1(emb_pos)))\n",
    "        encoded_emb_neg = norm_layer(layer2(layer1(emb_neg)))\n",
    "\n",
    "        pos_dist = Lambda(euclidean_distance, name='pos_dist')([encoded_emb, encoded_emb_pos])\n",
    "        neg_dist = Lambda(euclidean_distance, name='neg_dist')([encoded_emb, encoded_emb_neg])\n",
    "\n",
    "        def cal_output_shape(input_shape):\n",
    "            shape = list(input_shape[0])\n",
    "            assert len(shape) == 2  # only valid for 2D tensors\n",
    "            shape[-1] *= 2\n",
    "            return tuple(shape)\n",
    "\n",
    "        stacked_dists = Lambda(\n",
    "            lambda vects: K.stack(vects, axis=1),\n",
    "            name='stacked_dists',\n",
    "            output_shape=cal_output_shape\n",
    "        )([pos_dist, neg_dist])\n",
    "        \n",
    "        self.model = Model([emb_anchor, emb_pos, emb_neg], stacked_dists, name='triple_siamese')\n",
    "        self.model.compile(loss=triplet_loss, optimizer='adam', metrics=[accuracy])\n",
    "        self.infer = Model(inputs=self.model.get_layer('anchor_input').get_input_at(0), \n",
    "                           outputs=self.model.get_layer('norm_layer').get_output_at(0))\n",
    "        self.early = EarlyStopping('val_loss', patience = 3)\n",
    "        self.checkpoint = ModelCheckpoint(self.save_path, 'val_loss', save_best_only=True, save_weights_only = True)\n",
    "        \n",
    "        \n",
    "    def train(self, X, retrain = True):\n",
    "        if retrain:\n",
    "            n_triplets = len(X[0])\n",
    "            self.model.fit(X, np.ones((n_triplets, 2)), batch_size=800, epochs=200, shuffle=True, validation_split=0.2, callbacks = [self.early, self.checkpoint])\n",
    "        else:\n",
    "            self.load()\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.infer.predict(X)\n",
    "    \n",
    "    def save(self):\n",
    "        self.model.save_weights(self.save_path)\n",
    "        \n",
    "    def load(self):\n",
    "        self.model.load_weights(self.save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "l2_normalize() got an unexpected keyword argument 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-93532bddc794>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./output'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexist_ok\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGlobalModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0massignments_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpubs_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpubs_validate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpubs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mread_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweighted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweighted_embedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-69a535a8d112>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    229\u001b[0m         \u001b[0mnorm_layer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLambda\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml2Norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'norm_layer'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 231\u001b[1;33m         \u001b[0mencoded_emb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnorm_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb_anchor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    232\u001b[0m         \u001b[0mencoded_emb_pos\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnorm_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb_pos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[0mencoded_emb_neg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnorm_layer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0memb_neg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\engine\\base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    455\u001b[0m             \u001b[1;31m# Actually call the layer,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m             \u001b[1;31m# collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 457\u001b[1;33m             \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    458\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\layers\\core.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, inputs, mask)\u001b[0m\n\u001b[0;32m    680\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhas_arg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'mask'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m             \u001b[0marguments\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'mask'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 682\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0marguments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    683\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcompute_mask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-69a535a8d112>\u001b[0m in \u001b[0;36ml2Norm\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[1;31m## Triplet Model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0ml2Norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2_normalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0meuclidean_distance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvects\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Program Files (x86)\\Microsoft Visual Studio\\Shared\\Anaconda3_64\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36ml2_normalize\u001b[1;34m(x, axis)\u001b[0m\n\u001b[0;32m   3379\u001b[0m         \u001b[0mA\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3380\u001b[0m     \"\"\"\n\u001b[1;32m-> 3381\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0ml2_normalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3382\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3383\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: l2_normalize() got an unexpected keyword argument 'axis'"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    os.makedirs('./output', exist_ok = True)\n",
    "    m = GlobalModel()\n",
    "    assignments_train, pubs_train, pubs_validate, pubs = read_data()\n",
    "    _, _, _, weighted = weighted_embedding()\n",
    "    emb, emb_pos, emb_neg = gen_triple(weighted, 2000000)\n",
    "    print('gen triple done!')\n",
    "    \n",
    "    #m.train([emb, emb_pos, emb_neg], retrain = False)\n",
    "    m.train([emb, emb_pos, emb_neg], retrain = True)\n",
    "\n",
    "    all_id = [p['id'] for k, papers in pubs.items() for p in papers]\n",
    "    X = np.array( [ weighted[id] for id in all_id ] )\n",
    "    Y = m.predict(X)\n",
    "    d = dict(zip(all_id, Y))\n",
    "    pkl.dump(d, open(global_embedding_path,'wb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
