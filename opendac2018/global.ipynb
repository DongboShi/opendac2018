{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#coding=utf-8\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy import sparse\n",
    "import multiprocessing as mlp\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.metrics import calinski_harabaz_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.stem.porter import PorterStemmer  #todo: 还有其他词干抽取器\n",
    "from collections import defaultdict\n",
    "import math\n",
    "import pickle as pkl\n",
    "import gc\n",
    "import os\n",
    "from itertools import chain\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Lambda\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import *\n",
    "\n",
    "assignments_train_path = './data/assignment_train.json'\n",
    "pubs_train_path = './data/pubs_train.json'\n",
    "pubs_validate_path = './data/pubs_validate.json'\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='7'\n",
    "EMBEDDING_DIM = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "assignments_train = json.load(open(assignments_train_path,'r'))\n",
    "pubs_train = json.load(open(pubs_train_path, 'r'))\n",
    "pubs_validate = json.load(open(pubs_validate_path,'r'))\n",
    "pubs={**pubs_train, **pubs_validate}\n",
    "assert(len(pubs)==len(pubs_train)+len(pubs_validate))\n",
    "\n",
    "stop_words_set = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def clean_name(nm):\n",
    "    return re.sub('[^a-z]', '', nm.lower())\n",
    "\n",
    "def is_same_name(s1, s2):\n",
    "    return clean_name(s1)==clean_name(s2)\n",
    "\n",
    "def clean_sent(s):\n",
    "    words = re.sub('[^ \\-_a-z]', ' ', s.lower()).split()\n",
    "    stemer = PorterStemmer()\n",
    "    return [stemer.stem(w) for w in words]\n",
    "    \n",
    "def ExtractTxt(doc, primary_author):\n",
    "    \"\"\"\n",
    "    把一个文档变为：\n",
    "    [题目，合作者(姓名,组织)，期刊，摘要，关键词]\n",
    "    各种预处理之后的word list\n",
    "    \"\"\"\n",
    "    title = clean_sent(doc['title']) if doc.get('title',None) else []\n",
    "    venue = clean_sent(doc['venue']) if doc.get('venue',None) else []\n",
    "    abstract = clean_sent(doc['abstract']) if doc.get('abstract',None) else []\n",
    "    keywords = clean_sent(' '.join(doc['keywords'])) if doc.get('keywords',None) else []\n",
    "    coauthors = []\n",
    "    if doc.get('authors',None):\n",
    "        for aut in doc['authors']:\n",
    "            if not is_same_name(aut.get('name',''), primary_author):\n",
    "                coauthors.append( clean_name(aut.get('name','')) )\n",
    "                coauthors.extend( clean_sent(aut.get('org','')) )\n",
    "    return title+coauthors+venue+abstract+keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "material = []\n",
    "paper_id = []\n",
    "pool = mlp.Pool(20)\n",
    "for k,v in pubs.items():\n",
    "    material.extend(pool.starmap( ExtractTxt, zip( v, [k]*len(v) ) ))\n",
    "    paper_id.extend( [doc['id'] for doc in v])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "model = Word2Vec(material, size=EMBEDDING_DIM, window=5, min_count=5, workers=20)\n",
    "pkl.dump(dict(zip(paper_id, material)), open('material.pkl','wb'))\n",
    "model.save('word.emb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Weighted Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#todo: 并行\n",
    "def calc_idf(material):\n",
    "    cnt = defaultdict(int)\n",
    "    idf = {}\n",
    "    for doc in material:\n",
    "        for word in doc:\n",
    "            cnt[word]+=1\n",
    "    for k,v in cnt.items():\n",
    "        idf[k] = math.log( len(material)/v )\n",
    "    return idf\n",
    "\n",
    "def project_embedding(docs, wv, idf):\n",
    "    wei_embed = {}\n",
    "\n",
    "    for id, doc in docs.items():\n",
    "        word_vecs = []\n",
    "        sum_weight = 0.0\n",
    "        for word in doc:\n",
    "            if word in wv and word in idf:\n",
    "                word_vecs.append( wv[word] * idf[word] )\n",
    "                sum_weight += idf[word]\n",
    "        wei_embed[id] = np.sum(word_vecs, axis = 0) / sum_weight\n",
    "        \n",
    "    return wei_embed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = pkl.load(open('material.pkl','rb'))\n",
    "model = Word2Vec.load('word.emb')\n",
    "idf = calc_idf(material)\n",
    "weighted = project_embedding(docs, model.wv, idf)\n",
    "gc.collect()\n",
    "\n",
    "#Here we obtain X_i, 这部分还算快。\n",
    "#warning: 加权结果可能有点大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Triplet Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_neg_id(all_papers, excludes):\n",
    "    while True:\n",
    "        i = np.random.choice(len(all_papers))\n",
    "        if all_papers[i] not in excludes:\n",
    "            return all_papers[i]\n",
    "\n",
    "def gen_triple(weighted, sz = 1000000):\n",
    "    if os.path.exists(\"triple.pkl\"):\n",
    "        d = pkl.load(open(\"triple.pkl\",'rb'))\n",
    "        return d['emb'], d['emb_pos'], d['emb_neg']\n",
    "    \n",
    "    triples = []\n",
    "    authors = list(assignments_train.keys())\n",
    "    all_papers = list(set([p['id'] for k,v in pubs_train.items() for p in v]))\n",
    "    I = 0\n",
    "    try:\n",
    "        while True:\n",
    "            author_papers = assignments_train[ authors[I] ]\n",
    "            I+=1\n",
    "            if I>=len(authors):\n",
    "                I=0\n",
    "            for clust in author_papers:\n",
    "                if len(clust)<=1:\n",
    "                    continue\n",
    "                for pid in clust:\n",
    "                    sam = np.random.choice(clust,  min(len(clust), 5), replace=False)  #因为平均簇大小是5\n",
    "                    for pid_pos in sam:\n",
    "                        triples.append( [pid, pid_pos, get_neg_id(all_papers, clust)] )\n",
    "                        if len(triples)>=sz:\n",
    "                            raise StopIteration\n",
    "    except StopIteration as e:\n",
    "        print(len(triples))\n",
    "    \n",
    "    emb = np.array([ weighted[t[0]] for t in triples ])\n",
    "    emb_pos = np.array([ weighted[t[1]] for t in triples ])\n",
    "    emb_neg = np.array([ weighted[t[2]] for t in triples ])\n",
    "    pkl.dump({'emb':emb, 'emb_pos':emb_pos, 'emb_neg':emb_neg}, open(\"triple.pkl\",'wb'))\n",
    "    return emb, emb_pos, emb_neg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def l2Norm(x):\n",
    "    return K.l2_normalize(x, axis=-1)\n",
    "\n",
    "def euclidean_distance(vects):\n",
    "    x, y = vects\n",
    "    return K.sqrt(K.maximum(K.sum(K.square(x - y), axis=1, keepdims=True), K.epsilon()))\n",
    "\n",
    "def triplet_loss(_, y_pred):\n",
    "    margin = K.constant(1)\n",
    "    return K.mean(K.maximum(K.constant(0), K.square(y_pred[:,0,0]) - K.square(y_pred[:,1,0]) + margin))\n",
    "\n",
    "def accuracy(_, y_pred):\n",
    "    return K.mean(y_pred[:,0,0] < y_pred[:,1,0])\n",
    "\n",
    "class GlobalModel(object):\n",
    "    def __init__(self):\n",
    "        self.save_path = 'GlobalModel.h5'\n",
    "        emb_anchor = Input(shape=(EMB_DIM, ), name='anchor_input')\n",
    "        emb_pos = Input(shape=(EMB_DIM, ), name='pos_input')\n",
    "        emb_neg = Input(shape=(EMB_DIM, ), name='neg_input')\n",
    "\n",
    "        # shared layers\n",
    "        layer1 = Dense(128, activation='relu', name='first_emb_layer')\n",
    "        layer2 = Dense(64, activation='relu', name='last_emb_layer')\n",
    "        norm_layer = Lambda(l2Norm, name='norm_layer', output_shape=[64])\n",
    "\n",
    "        encoded_emb = norm_layer(layer2(layer1(emb_anchor)))\n",
    "        encoded_emb_pos = norm_layer(layer2(layer1(emb_pos)))\n",
    "        encoded_emb_neg = norm_layer(layer2(layer1(emb_neg)))\n",
    "\n",
    "        pos_dist = Lambda(euclidean_distance, name='pos_dist')([encoded_emb, encoded_emb_pos])\n",
    "        neg_dist = Lambda(euclidean_distance, name='neg_dist')([encoded_emb, encoded_emb_neg])\n",
    "\n",
    "        def cal_output_shape(input_shape):\n",
    "            shape = list(input_shape[0])\n",
    "            assert len(shape) == 2  # only valid for 2D tensors\n",
    "            shape[-1] *= 2\n",
    "            return tuple(shape)\n",
    "\n",
    "        stacked_dists = Lambda(\n",
    "            lambda vects: K.stack(vects, axis=1),\n",
    "            name='stacked_dists',\n",
    "            output_shape=cal_output_shape\n",
    "        )([pos_dist, neg_dist])\n",
    "        \n",
    "        self.model = Model([emb_anchor, emb_pos, emb_neg], stacked_dists, name='triple_siamese')\n",
    "        self.model.compile(loss=triplet_loss, optimizer=Adam(lr=0.01), metrics=[accuracy])\n",
    "        self.infer = Model(inputs=model.get_input_at(0), outputs=model.get_layer('norm_layer').get_output_at(0))\n",
    "        \n",
    "        \n",
    "    def train(self, X):\n",
    "        n_triplets = len(X[0])\n",
    "        self.model.fit(X, np.ones((n_triplets, 2)), batch_size=64, epochs=5, shuffle=True, validation_split=0.2)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        return self.infer.predict(X)\n",
    "    \n",
    "    def save(self):\n",
    "        self.model.save_weights(self.save_path)\n",
    "        \n",
    "    def load(self):\n",
    "        self.model.load_weights(self.save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000000\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'GlobalModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-14fd78d621e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_neg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_triple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweighted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGlobalModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0memb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_pos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0memb_neg\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'GlobalModel' is not defined"
     ]
    }
   ],
   "source": [
    "emb, emb_pos, emb_neg = gen_triple(weighted)\n",
    "m = GlobalModel()\n",
    "m.train([emb, emb_pos, emb_neg])\n",
    "m.save()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
